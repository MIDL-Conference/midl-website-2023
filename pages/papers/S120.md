---
title: "Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models"
page_class: "paper"
---

{% from "_macros.html" import presentation, button, teaser, youtube %}

# S120 - Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models

#### Aakash Mishra, Rajat Mittal, Christy Jestin, Kostas Tingos, Pranav Rajpurkar

[% .details %]
<a class="toggle_visibility" data-selector=".abstract" data-level="3">Show abstract</a>
- <a class="toggle_visibility" data-selector=".schedule" data-level="3">Show schedule</a>
- <a href="https://openreview.net/pdf?id=tQvYo-DMrO">PDF</a>
- <a href="https://openreview.net/forum?id=tQvYo-DMrO">Reviews</a>

<p>
    <span class="abstract">
        Recent advancements in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Domain pre-trained models, such as CXR-BERT, BlueBERT, and ClinicalBERT, offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model\'s alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maximal performance.
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide abstract</a></span>
    </span>
</p>

<p>
    <span class="schedule">
        Wednesday, July 12: Virtual poster session - 8:00â€“9:00<br>
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide schedule</a></span>
    </span>
</p>
[% / %]

---


### Short paper
