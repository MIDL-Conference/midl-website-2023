---
title: "Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation"
page_class: "paper"
---

{% from "_macros.html" import presentation, button, teaser, youtube %}

# P165 - Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation

#### Md Mostafijur Rahman, Radu Marculescu


[% .details %]
<a class="toggle_visibility" data-selector=".abstract" data-level="3">Show abstract</a>
- <a href="https://openreview.net/pdf?id=u0MHV19E2n">PDF</a>
- <a href="https://openreview.net/forum?id=u0MHV19E2n">Reviews</a>

<p>
    <span class="abstract">
        Transformers have shown great success in medical image segmentation. However, transformers may exhibit a limited generalization ability due to the underlying single-scale self-attention (SA) mechanism. In this paper, we address this issue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT) backbone network, which improves the generalizability of the model by computing SA at multiple scales. We also incorporate an attention-based decoder, namely Cascaded Attention Decoding (CASCADE), for further refinement of multi-stage features generated by MERIT. Finally, we introduce an effective multi-stage feature mixing loss aggregation (MUTATION) method for better model training via implicit ensembling. Our experiments on two widely used medical image segmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate the superior performance of MERIT over state-of-the-art methods. Our MERIT architecture and MUTATION loss aggregation can be used with downstream medical image and semantic segmentation tasks.
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide abstract</a></span>
    </span>
</p>
[% / %]

---


### Poster presentation

**Schedule**: Tuesday, July 11: Posters — 10:30–12:00 & 15:00–16:00<br>
**Poster location**: T26

{{ presentation('DYwsK2lmhm4', '/virtual/poster/P165.pdf', 720, 450) }}