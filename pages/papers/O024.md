---
title: "Vision-Language Modelling For Radiological Imaging and Reports In The Low Data Regime"
page_class: "paper"
---

{% from "_macros.html" import presentation, button, teaser, youtube %}

# O024 - Vision-Language Modelling For Radiological Imaging and Reports In The Low Data Regime

#### Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman


[% .details %]
<a class="toggle_visibility" data-selector=".abstract" data-level="3">Show abstract</a>
- <a href="https://openreview.net/pdf?id=2XVITHcQCfj">PDF</a>
- <a href="https://openreview.net/forum?id=2XVITHcQCfj">Reviews</a>

<p>
    <span class="abstract">
        This paper explores training medical vision-language models (VLMs) -- where the visual and language inputs are embedded into a common space -- with a particular focus on scenarios where training data is limited, as is often the case in clinical datasets. We explore several candidate methods to improve low-data performance, including: (i) adapting generic pre-trained models to novel image and text domains (i.e.\ medical imaging and reports) via unimodal self-supervision; (ii) using local (e.g.\ GLoRIA) \& global (e.g. InfoNCE) contrastive loss functions as well as a combination of the two; (iii) extra supervision during VLM training, via: (a) image- and text-only self-supervision, and (b) creating additional positive image-text pairs for training through augmentation and nearest-neighbour search.  Using text-to-image retrieval as a benchmark, we evaluate the performance of these methods with variable sized training datasets of paired chest X-rays and radiological reports.  Combined, they significantly improve retrieval compared to fine-tuning CLIP, roughly equivalent to training with $10\times$ the data. A similar pattern is found in the downstream task classification of CXR-related conditions with our method outperforming CLIP and also BioVIL, a strong CXR VLM benchmark, in the zero-shot and linear probing settings. We conclude with a set of recommendations for researchers aiming to train vision-language models on other medical imaging modalities when training data is scarce. To facilitate further research, we will make our code and models publicly available.  
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide abstract</a></span>
    </span>
</p>
[% / %]

---


### Oral presentation

**Schedule**: Tuesday, July 11: Oral session 5 - Semi-supervised/self-supervised methods — 14:00–15:00<br>Tuesday, July 11: Posters — 10:30–12:00 & 15:00–16:00<br>
**Poster location**: T08

